{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Loading\n",
    "# --------------------\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load all relevant datasets.\"\"\"\n",
    "    discharge_diagnosis = pd.read_csv('../data/raw/discharge_diagnosis.csv')\n",
    "    discharge_procedures = pd.read_csv('../data/raw/discharge_procedures.csv')\n",
    "    history_of_present_illness = pd.read_csv('../data/raw/history_of_present_illness.csv')\n",
    "    icd_diagnosis = pd.read_csv('../data/raw/icd_diagnosis.csv')\n",
    "    icd_procedures = pd.read_csv('../data/raw/icd_procedures.csv')\n",
    "    lab_test_mapping = pd.read_csv('../data/raw/lab_test_mapping.csv')\n",
    "    laboratory_tests = pd.read_csv('../data/raw/laboratory_tests.csv')\n",
    "    microbiology = pd.read_csv('../data/raw/microbiology.csv')\n",
    "    physical_examination = pd.read_csv('../data/raw/physical_examination.csv')\n",
    "    radiology_reports = pd.read_csv('../data/raw/radiology_reports.csv')\n",
    "\n",
    "    with open('../data/raw/pathology_ids.json') as f:\n",
    "        pathology_ids = json.load(f)\n",
    "\n",
    "    return {\n",
    "        \"discharge_diagnosis\": discharge_diagnosis,\n",
    "        \"discharge_procedures\": discharge_procedures,\n",
    "        \"hpi\": history_of_present_illness,\n",
    "        \"icd_diagnosis\": icd_diagnosis,\n",
    "        \"icd_procedures\": icd_procedures,\n",
    "        \"lab_test_mapping\": lab_test_mapping,\n",
    "        \"lab_tests\": laboratory_tests,\n",
    "        \"microbiology\": microbiology,\n",
    "        \"physical_examination\": physical_examination,\n",
    "        \"radiology_reports\": radiology_reports,\n",
    "        \"pathology_ids\": pathology_ids\n",
    "}\n",
    "\n",
    "# Load data and check\n",
    "data = load_data()\n",
    "for key, df in data.items():\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        print(f\"{key}: {df.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: Loaded JSON data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "# --------------------------\n",
    "\n",
    "def merge_data(data):\n",
    "    \"\"\"\n",
    "    Merge all clinical data into a single DataFrame based on 'hadm_id'.\n",
    "    \"\"\"\n",
    "    merged_data = data['discharge_diagnosis'].merge(data['hpi'], on='hadm_id', how='inner') \\\n",
    "                                             .merge(data['physical_examination'], on='hadm_id', how='inner') \\\n",
    "                                             .merge(data['lab_tests'], on='hadm_id', how='inner') \\\n",
    "                                             .merge(data['microbiology'], on='hadm_id', how='inner') \\\n",
    "                                             .merge(data['radiology_reports'], on='hadm_id', how='inner')\n",
    "    return merged_data\n",
    "\n",
    "def filter_by_pathologies(data, pathology_ids):\n",
    "    \"\"\"\n",
    "    Filter data based on specific pathology IDs.\n",
    "    \"\"\"\n",
    "    pathology_ids_list = [item for sublist in pathology_ids.values() for item in sublist]\n",
    "    return data[data['hadm_id'].isin(pathology_ids_list)]\n",
    "\n",
    "def handle_missing_values(data):\n",
    "    \"\"\"\n",
    "    Handle missing data by filling forward.\n",
    "    \"\"\"\n",
    "    return data.fillna(method='ffill')\n",
    "\n",
    "def preprocess_data(data, pathology_ids):\n",
    "    \"\"\"\n",
    "    Preprocess and combine data steps.\n",
    "    \"\"\"\n",
    "    combined_data = merge_data(data)\n",
    "    filtered_data = filter_by_pathologies(combined_data, pathology_ids)\n",
    "    cleaned_data = handle_missing_values(filtered_data)\n",
    "    return cleaned_data\n",
    "\n",
    "# Preprocess data\n",
    "processed_data = preprocess_data(data, data['pathology_ids'])\n",
    "print(f\"Processed Data Shape: {processed_data.shape}\")\n",
    "processed_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Engineering\n",
    "# ---------------------------\n",
    "\n",
    "def create_text_features(data):\n",
    "    \"\"\"\n",
    "    Create features based on text length of 'hpi'.\n",
    "    \"\"\"\n",
    "    if 'hpi' in data.columns:\n",
    "        data['hpi_length'] = data['hpi'].apply(lambda x: len(str(x).split()))\n",
    "    return data\n",
    "\n",
    "def encode_categorical_features(data):\n",
    "    \"\"\"\n",
    "    One-hot encode radiology modality and ICD codes.\n",
    "    \"\"\"\n",
    "    if 'modality' in data.columns:\n",
    "        data = pd.get_dummies(data, columns=['modality'], prefix='modality')\n",
    "    if 'icd_code' in data.columns:\n",
    "        data = pd.get_dummies(data, columns=['icd_code'], prefix='icd')\n",
    "    return data\n",
    "\n",
    "def feature_engineering(data):\n",
    "    \"\"\"\n",
    "    Apply feature engineering transformations.\n",
    "    \"\"\"\n",
    "    data = create_text_features(data)\n",
    "    data = encode_categorical_features(data)\n",
    "    return data\n",
    "\n",
    "# Apply feature engineering\n",
    "feature_data = feature_engineering(processed_data)\n",
    "feature_data.to_csv('../data/processed/feature_matrix.csv', index=False)\n",
    "print(f\"Feature Data Shape: {feature_data.shape}\")\n",
    "feature_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "def measure_time_cpu(matrix_a, matrix_b):\n",
    "    start_time = time.time()\n",
    "    result = tf.matmul(matrix_a, matrix_b)\n",
    "    tf.experimental.numpy.copy(result)  # Ensure completion of the computation\n",
    "    return time.time() - start_time\n",
    "\n",
    "def measure_time_gpu(matrix_a, matrix_b):\n",
    "    # Warm-up runs\n",
    "    for _ in range(5):\n",
    "        _ = tf.matmul(matrix_a, matrix_b)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = tf.matmul(matrix_a, matrix_b)\n",
    "    tf.experimental.numpy.copy(result)  # Ensure completion of the computation\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Generate random matrices\n",
    "matrix_a = tf.random.normal(shape=(10000, 10000))\n",
    "matrix_b = tf.random.normal(shape=(10000, 10000))\n",
    "\n",
    "# Measure CPU time\n",
    "cpu_time = measure_time_cpu(matrix_a, matrix_b)\n",
    "print(\"Average time on CPU:\", cpu_time)\n",
    "\n",
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Ensure matrices are on the GPU\n",
    "    matrix_a_gpu = tf.constant(matrix_a)\n",
    "    matrix_b_gpu = tf.constant(matrix_b)\n",
    "\n",
    "    # Measure GPU time\n",
    "    gpu_time = measure_time_gpu(matrix_a_gpu, matrix_b_gpu)\n",
    "    print(\"Average time on GPU:\", gpu_time)\n",
    "\n",
    "    # Calculate and print the performance difference\n",
    "    performance_improvement = ((cpu_time - gpu_time) / cpu_time) * 100\n",
    "    print(f\"GPU is {performance_improvement:.2f}% faster than CPU.\")\n",
    "else:\n",
    "    print(\"GPU not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Training\n",
    "# ----------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# def train_random_forest(X, y):\n",
    "#     \"\"\"\n",
    "#     Train a RandomForest model.\n",
    "#     \"\"\"\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#     model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     report = classification_report(y_test, y_pred)\n",
    "#     return model, accuracy, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping non-numeric columns: Index(['hpi', 'pe', 'valuestr_x', 'valuestr_y', 'note_id', 'region',\n",
      "       'exam_name', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load feature data\n",
    "feature_data = pd.read_csv('../data/processed/feature_matrix.csv')\n",
    "X = feature_data.drop(columns=['discharge_diagnosis', 'hadm_id'])  # Replace 'outcome' with actual target variable\n",
    "y = feature_data['discharge_diagnosis']  # Replace 'outcome' with actual target variable\n",
    "\n",
    "# Identify and drop non-numeric columns from X\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "print(f\"Dropping non-numeric columns: {non_numeric_columns}\")\n",
    "X = X.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load and split your data (assuming X and y are already defined)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode string labels to integers\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# Create an XGBoost classifier with GPU support and optimized parameters\n",
    "model = xgb.XGBClassifier(\n",
    "    tree_method='gpu_hist',  # Use GPU-accelerated training\n",
    "    use_label_encoder=False,\n",
    "    max_bin=256,  # Reduce bins to save memory\n",
    "    subsample=0.8,  # Subsample 80% of the data\n",
    "    colsample_bytree=0.8,  # Use 80% of features for each tree\n",
    "    n_estimators=50,  # Reduce the number of trees\n",
    "    max_depth=6  # Limit tree depth\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Transform predictions back to original string labels (optional, for interpretation)\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "\n",
    "# Delete variables you no longer need\n",
    "del X_train, X_test, y_train, y_test  # Adjust according to your variable names\n",
    "gc.collect()  # Force garbage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import gc\n",
    "\n",
    "# Load and split your data (ensure it's reduced or optimized)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reduce memory usage\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Remove constant columns if applicable\n",
    "constant_columns = [col for col in X_train.columns if X_train[col].nunique() <= 1]\n",
    "if constant_columns:\n",
    "    print(f\"Removing constant columns: {constant_columns}\")\n",
    "    X_train = X_train.drop(columns=constant_columns)\n",
    "    X_test = X_test.drop(columns=constant_columns)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "\n",
    "# Train LightGBM model with parameters to reduce memory consumption\n",
    "model = lgb.LGBMClassifier(device='gpu', max_bin=256, n_estimators=100, max_depth=6)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Ensure y_train and y_test are integers\n",
    "if y_train.dtype == 'object' or y_test.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Improved neural network with additional layers, dropout, and batch normalization\n",
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, len(torch.unique(y_train)))  # Adjust output size based on number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ImprovedNN(X_train.shape[1]).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).argmax(dim=1)\n",
    "    accuracy = (predictions == y_test).float().mean().item()\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, '../models/random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model again\n",
    "model, accuracy, report = train_random_forest(X, y)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, '../models/random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: SHAP Analysis\n",
    "# ---------------------\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_shap_analysis(model, X):\n",
    "    \"\"\"\n",
    "    Compute SHAP values for feature importance analysis.\n",
    "    \"\"\"\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    return explainer, shap_values\n",
    "\n",
    "# Load model and data\n",
    "model = joblib.load('../models/random_forest_model.pkl')\n",
    "X = pd.read_csv('../data/processed/feature_matrix.csv').drop(columns=['discharge_diagnosis', 'hadm_id'])  # Adjust as needed\n",
    "\n",
    "explainer, shap_values = run_shap_analysis(model, X)\n",
    "\n",
    "# Visualize SHAP values\n",
    "plt.title(\"SHAP Summary Plot\")\n",
    "shap.summary_plot(shap_values[1], X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
