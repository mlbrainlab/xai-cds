{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discharge_diagnosis: (2400, 2)\n",
      "discharge_procedures: (2122, 2)\n",
      "hpi: (2400, 2)\n",
      "icd_diagnosis: (17357, 2)\n",
      "icd_procedures: (2917, 4)\n",
      "lab_test_mapping: (1209, 6)\n",
      "lab_tests: (138788, 5)\n",
      "microbiology: (4403, 4)\n",
      "physical_examination: (2400, 2)\n",
      "radiology_reports: (5960, 6)\n",
      "pathology_ids: Loaded JSON data\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Loading\n",
    "# --------------------\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load all relevant datasets.\"\"\"\n",
    "    discharge_diagnosis = pd.read_csv('../data/raw/discharge_diagnosis.csv')\n",
    "    discharge_procedures = pd.read_csv('../data/raw/discharge_procedures.csv')\n",
    "    history_of_present_illness = pd.read_csv('../data/raw/history_of_present_illness.csv')\n",
    "    icd_diagnosis = pd.read_csv('../data/raw/icd_diagnosis.csv')\n",
    "    icd_procedures = pd.read_csv('../data/raw/icd_procedures.csv')\n",
    "    lab_test_mapping = pd.read_csv('../data/raw/lab_test_mapping.csv')\n",
    "    laboratory_tests = pd.read_csv('../data/raw/laboratory_tests.csv')\n",
    "    microbiology = pd.read_csv('../data/raw/microbiology.csv')\n",
    "    physical_examination = pd.read_csv('../data/raw/physical_examination.csv')\n",
    "    radiology_reports = pd.read_csv('../data/raw/radiology_reports.csv')\n",
    "\n",
    "    with open('../data/raw/pathology_ids.json') as f:\n",
    "        pathology_ids = json.load(f)\n",
    "\n",
    "    return {\n",
    "        \"discharge_diagnosis\": discharge_diagnosis,\n",
    "        \"discharge_procedures\": discharge_procedures,\n",
    "        \"hpi\": history_of_present_illness,\n",
    "        \"icd_diagnosis\": icd_diagnosis,\n",
    "        \"icd_procedures\": icd_procedures,\n",
    "        \"lab_test_mapping\": lab_test_mapping,\n",
    "        \"lab_tests\": laboratory_tests,\n",
    "        \"microbiology\": microbiology,\n",
    "        \"physical_examination\": physical_examination,\n",
    "        \"radiology_reports\": radiology_reports,\n",
    "        \"pathology_ids\": pathology_ids\n",
    "}\n",
    "\n",
    "# Load data and check\n",
    "data = load_data()\n",
    "for key, df in data.items():\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        print(f\"{key}: {df.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: Loaded JSON data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3213788/3546471226.py:26: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  return data.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Data Shape: (1941697, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>discharge_diagnosis</th>\n",
       "      <th>hpi</th>\n",
       "      <th>pe</th>\n",
       "      <th>itemid</th>\n",
       "      <th>valuestr_x</th>\n",
       "      <th>ref_range_lower</th>\n",
       "      <th>ref_range_upper</th>\n",
       "      <th>test_itemid</th>\n",
       "      <th>valuestr_y</th>\n",
       "      <th>spec_itemid</th>\n",
       "      <th>note_id</th>\n",
       "      <th>modality</th>\n",
       "      <th>region</th>\n",
       "      <th>exam_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51085</td>\n",
       "      <td>NEGATIVE.  FOR QUANTITATION OF POSITIVES, SEND...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90039</td>\n",
       "      <td>&lt;10,000 organisms/ml.</td>\n",
       "      <td>70079</td>\n",
       "      <td>10040626-RR-9</td>\n",
       "      <td>CT</td>\n",
       "      <td>Abdomen</td>\n",
       "      <td>CT ABD &amp; PELVIS WITH CONTRAST</td>\n",
       "      <td>EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51085</td>\n",
       "      <td>NEGATIVE.  FOR QUANTITATION OF POSITIVES, SEND...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90201</td>\n",
       "      <td>NO GROWTH.</td>\n",
       "      <td>70012</td>\n",
       "      <td>10040626-RR-9</td>\n",
       "      <td>CT</td>\n",
       "      <td>Abdomen</td>\n",
       "      <td>CT ABD &amp; PELVIS WITH CONTRAST</td>\n",
       "      <td>EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51514</td>\n",
       "      <td>NEG.</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90039</td>\n",
       "      <td>&lt;10,000 organisms/ml.</td>\n",
       "      <td>70079</td>\n",
       "      <td>10040626-RR-9</td>\n",
       "      <td>CT</td>\n",
       "      <td>Abdomen</td>\n",
       "      <td>CT ABD &amp; PELVIS WITH CONTRAST</td>\n",
       "      <td>EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51514</td>\n",
       "      <td>NEG.</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90201</td>\n",
       "      <td>NO GROWTH.</td>\n",
       "      <td>70012</td>\n",
       "      <td>10040626-RR-9</td>\n",
       "      <td>CT</td>\n",
       "      <td>Abdomen</td>\n",
       "      <td>CT ABD &amp; PELVIS WITH CONTRAST</td>\n",
       "      <td>EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51508</td>\n",
       "      <td>Straw.</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90039</td>\n",
       "      <td>&lt;10,000 organisms/ml.</td>\n",
       "      <td>70079</td>\n",
       "      <td>10040626-RR-9</td>\n",
       "      <td>CT</td>\n",
       "      <td>Abdomen</td>\n",
       "      <td>CT ABD &amp; PELVIS WITH CONTRAST</td>\n",
       "      <td>EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hadm_id discharge_diagnosis  \\\n",
       "0  20890008  acute appendicitis   \n",
       "1  20890008  acute appendicitis   \n",
       "2  20890008  acute appendicitis   \n",
       "3  20890008  acute appendicitis   \n",
       "4  20890008  acute appendicitis   \n",
       "\n",
       "                                                 hpi  \\\n",
       "0  ___ with no significant PMH presenting with ac...   \n",
       "1  ___ with no significant PMH presenting with ac...   \n",
       "2  ___ with no significant PMH presenting with ac...   \n",
       "3  ___ with no significant PMH presenting with ac...   \n",
       "4  ___ with no significant PMH presenting with ac...   \n",
       "\n",
       "                                                  pe  itemid  \\\n",
       "0  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51085   \n",
       "1  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51085   \n",
       "2  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51514   \n",
       "3  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51514   \n",
       "4  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51508   \n",
       "\n",
       "                                          valuestr_x  ref_range_lower  \\\n",
       "0  NEGATIVE.  FOR QUANTITATION OF POSITIVES, SEND...              NaN   \n",
       "1  NEGATIVE.  FOR QUANTITATION OF POSITIVES, SEND...              NaN   \n",
       "2                                               NEG.              0.2   \n",
       "3                                               NEG.              0.2   \n",
       "4                                             Straw.              0.2   \n",
       "\n",
       "   ref_range_upper  test_itemid             valuestr_y  spec_itemid  \\\n",
       "0              NaN        90039  <10,000 organisms/ml.        70079   \n",
       "1              NaN        90201             NO GROWTH.        70012   \n",
       "2              1.0        90039  <10,000 organisms/ml.        70079   \n",
       "3              1.0        90201             NO GROWTH.        70012   \n",
       "4              1.0        90039  <10,000 organisms/ml.        70079   \n",
       "\n",
       "         note_id modality   region                      exam_name  \\\n",
       "0  10040626-RR-9       CT  Abdomen  CT ABD & PELVIS WITH CONTRAST   \n",
       "1  10040626-RR-9       CT  Abdomen  CT ABD & PELVIS WITH CONTRAST   \n",
       "2  10040626-RR-9       CT  Abdomen  CT ABD & PELVIS WITH CONTRAST   \n",
       "3  10040626-RR-9       CT  Abdomen  CT ABD & PELVIS WITH CONTRAST   \n",
       "4  10040626-RR-9       CT  Abdomen  CT ABD & PELVIS WITH CONTRAST   \n",
       "\n",
       "                                                text  \n",
       "0  EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...  \n",
       "1  EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...  \n",
       "2  EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...  \n",
       "3  EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...  \n",
       "4  EXAMINATION:\\nCT ABD AND PELVIS WITH CONTRAST:...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "# --------------------------\n",
    "\n",
    "def merge_data(data):\n",
    "    \"\"\"\n",
    "    Merge all clinical data into a single DataFrame based on 'hadm_id'.\n",
    "    \"\"\"\n",
    "    merged_data = data['discharge_diagnosis'].merge(data['hpi'], on='hadm_id', how='inner') \\\n",
    "                                             .merge(data['physical_examination'], on='hadm_id', how='inner') \\\n",
    "                                             .merge(data['lab_tests'], on='hadm_id', how='inner') \\\n",
    "                                             .merge(data['microbiology'], on='hadm_id', how='inner') \\\n",
    "                                             .merge(data['radiology_reports'], on='hadm_id', how='inner')\n",
    "    return merged_data\n",
    "\n",
    "def filter_by_pathologies(data, pathology_ids):\n",
    "    \"\"\"\n",
    "    Filter data based on specific pathology IDs.\n",
    "    \"\"\"\n",
    "    pathology_ids_list = [item for sublist in pathology_ids.values() for item in sublist]\n",
    "    return data[data['hadm_id'].isin(pathology_ids_list)]\n",
    "\n",
    "def handle_missing_values(data):\n",
    "    \"\"\"\n",
    "    Handle missing data by filling forward.\n",
    "    \"\"\"\n",
    "    return data.fillna(method='ffill')\n",
    "\n",
    "def preprocess_data(data, pathology_ids):\n",
    "    \"\"\"\n",
    "    Preprocess and combine data steps.\n",
    "    \"\"\"\n",
    "    combined_data = merge_data(data)\n",
    "    filtered_data = filter_by_pathologies(combined_data, pathology_ids)\n",
    "    cleaned_data = handle_missing_values(filtered_data)\n",
    "    return cleaned_data\n",
    "\n",
    "# Preprocess data\n",
    "processed_data = preprocess_data(data, data['pathology_ids'])\n",
    "print(f\"Processed Data Shape: {processed_data.shape}\")\n",
    "processed_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Data Shape: (1941697, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>discharge_diagnosis</th>\n",
       "      <th>hpi</th>\n",
       "      <th>pe</th>\n",
       "      <th>itemid</th>\n",
       "      <th>valuestr_x</th>\n",
       "      <th>ref_range_lower</th>\n",
       "      <th>ref_range_upper</th>\n",
       "      <th>test_itemid</th>\n",
       "      <th>valuestr_y</th>\n",
       "      <th>...</th>\n",
       "      <th>modality_Carotid ultrasound</th>\n",
       "      <th>modality_Drainage</th>\n",
       "      <th>modality_ERCP</th>\n",
       "      <th>modality_Fluoroscopy</th>\n",
       "      <th>modality_MRCP</th>\n",
       "      <th>modality_MRE</th>\n",
       "      <th>modality_MRI</th>\n",
       "      <th>modality_Radiograph</th>\n",
       "      <th>modality_Ultrasound</th>\n",
       "      <th>modality_Upper GI Series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51085</td>\n",
       "      <td>NEGATIVE.  FOR QUANTITATION OF POSITIVES, SEND...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90039</td>\n",
       "      <td>&lt;10,000 organisms/ml.</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51085</td>\n",
       "      <td>NEGATIVE.  FOR QUANTITATION OF POSITIVES, SEND...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90201</td>\n",
       "      <td>NO GROWTH.</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51514</td>\n",
       "      <td>NEG.</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90039</td>\n",
       "      <td>&lt;10,000 organisms/ml.</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51514</td>\n",
       "      <td>NEG.</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90201</td>\n",
       "      <td>NO GROWTH.</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20890008</td>\n",
       "      <td>acute appendicitis</td>\n",
       "      <td>___ with no significant PMH presenting with ac...</td>\n",
       "      <td>Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...</td>\n",
       "      <td>51508</td>\n",
       "      <td>Straw.</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90039</td>\n",
       "      <td>&lt;10,000 organisms/ml.</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    hadm_id discharge_diagnosis  \\\n",
       "0  20890008  acute appendicitis   \n",
       "1  20890008  acute appendicitis   \n",
       "2  20890008  acute appendicitis   \n",
       "3  20890008  acute appendicitis   \n",
       "4  20890008  acute appendicitis   \n",
       "\n",
       "                                                 hpi  \\\n",
       "0  ___ with no significant PMH presenting with ac...   \n",
       "1  ___ with no significant PMH presenting with ac...   \n",
       "2  ___ with no significant PMH presenting with ac...   \n",
       "3  ___ with no significant PMH presenting with ac...   \n",
       "4  ___ with no significant PMH presenting with ac...   \n",
       "\n",
       "                                                  pe  itemid  \\\n",
       "0  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51085   \n",
       "1  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51085   \n",
       "2  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51514   \n",
       "3  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51514   \n",
       "4  Temp: 97.6 HR: 46 BP: 106/65 RR: 18 100% Ra Ge...   51508   \n",
       "\n",
       "                                          valuestr_x  ref_range_lower  \\\n",
       "0  NEGATIVE.  FOR QUANTITATION OF POSITIVES, SEND...              NaN   \n",
       "1  NEGATIVE.  FOR QUANTITATION OF POSITIVES, SEND...              NaN   \n",
       "2                                               NEG.              0.2   \n",
       "3                                               NEG.              0.2   \n",
       "4                                             Straw.              0.2   \n",
       "\n",
       "   ref_range_upper  test_itemid             valuestr_y  ...  \\\n",
       "0              NaN        90039  <10,000 organisms/ml.  ...   \n",
       "1              NaN        90201             NO GROWTH.  ...   \n",
       "2              1.0        90039  <10,000 organisms/ml.  ...   \n",
       "3              1.0        90201             NO GROWTH.  ...   \n",
       "4              1.0        90039  <10,000 organisms/ml.  ...   \n",
       "\n",
       "   modality_Carotid ultrasound modality_Drainage modality_ERCP  \\\n",
       "0                        False             False         False   \n",
       "1                        False             False         False   \n",
       "2                        False             False         False   \n",
       "3                        False             False         False   \n",
       "4                        False             False         False   \n",
       "\n",
       "  modality_Fluoroscopy modality_MRCP  modality_MRE  modality_MRI  \\\n",
       "0                False         False         False         False   \n",
       "1                False         False         False         False   \n",
       "2                False         False         False         False   \n",
       "3                False         False         False         False   \n",
       "4                False         False         False         False   \n",
       "\n",
       "   modality_Radiograph  modality_Ultrasound  modality_Upper GI Series  \n",
       "0                False                False                     False  \n",
       "1                False                False                     False  \n",
       "2                False                False                     False  \n",
       "3                False                False                     False  \n",
       "4                False                False                     False  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Feature Engineering\n",
    "# ---------------------------\n",
    "\n",
    "def create_text_features(data):\n",
    "    \"\"\"\n",
    "    Create features based on text length of 'hpi'.\n",
    "    \"\"\"\n",
    "    if 'hpi' in data.columns:\n",
    "        data['hpi_length'] = data['hpi'].apply(lambda x: len(str(x).split()))\n",
    "    return data\n",
    "\n",
    "def encode_categorical_features(data):\n",
    "    \"\"\"\n",
    "    One-hot encode radiology modality and ICD codes.\n",
    "    \"\"\"\n",
    "    if 'modality' in data.columns:\n",
    "        data = pd.get_dummies(data, columns=['modality'], prefix='modality')\n",
    "    if 'icd_code' in data.columns:\n",
    "        data = pd.get_dummies(data, columns=['icd_code'], prefix='icd')\n",
    "    return data\n",
    "\n",
    "def feature_engineering(data):\n",
    "    \"\"\"\n",
    "    Apply feature engineering transformations.\n",
    "    \"\"\"\n",
    "    data = create_text_features(data)\n",
    "    data = encode_categorical_features(data)\n",
    "    return data\n",
    "\n",
    "# Apply feature engineering\n",
    "feature_data = feature_engineering(processed_data)\n",
    "feature_data.to_csv('../data/processed/feature_matrix.csv', index=False)\n",
    "print(f\"Feature Data Shape: {feature_data.shape}\")\n",
    "feature_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 01:49:56.475793: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-14 01:49:56.481813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731538196.488063 3213788 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731538196.489895 3213788 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-14 01:49:56.496712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time on CPU: 0.016513824462890625\n",
      "Average time on GPU: 5.1975250244140625e-05\n",
      "GPU is 99.69% faster than CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731538197.251946 3213788 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7457 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "def measure_time_cpu(matrix_a, matrix_b):\n",
    "    start_time = time.time()\n",
    "    result = tf.matmul(matrix_a, matrix_b)\n",
    "    tf.experimental.numpy.copy(result)  # Ensure completion of the computation\n",
    "    return time.time() - start_time\n",
    "\n",
    "def measure_time_gpu(matrix_a, matrix_b):\n",
    "    # Warm-up runs\n",
    "    for _ in range(5):\n",
    "        _ = tf.matmul(matrix_a, matrix_b)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = tf.matmul(matrix_a, matrix_b)\n",
    "    tf.experimental.numpy.copy(result)  # Ensure completion of the computation\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Generate random matrices\n",
    "matrix_a = tf.random.normal(shape=(10000, 10000))\n",
    "matrix_b = tf.random.normal(shape=(10000, 10000))\n",
    "\n",
    "# Measure CPU time\n",
    "cpu_time = measure_time_cpu(matrix_a, matrix_b)\n",
    "print(\"Average time on CPU:\", cpu_time)\n",
    "\n",
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Ensure matrices are on the GPU\n",
    "    matrix_a_gpu = tf.constant(matrix_a)\n",
    "    matrix_b_gpu = tf.constant(matrix_b)\n",
    "\n",
    "    # Measure GPU time\n",
    "    gpu_time = measure_time_gpu(matrix_a_gpu, matrix_b_gpu)\n",
    "    print(\"Average time on GPU:\", gpu_time)\n",
    "\n",
    "    # Calculate and print the performance difference\n",
    "    performance_improvement = ((cpu_time - gpu_time) / cpu_time) * 100\n",
    "    print(f\"GPU is {performance_improvement:.2f}% faster than CPU.\")\n",
    "else:\n",
    "    print(\"GPU not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Reducing data precision...\n",
      "Dropping non-numeric columns: Index(['hpi', 'pe', 'valuestr_x', 'valuestr_y', 'note_id', 'region',\n",
      "       'exam_name', 'text'],\n",
      "      dtype='object')\n",
      "Applying feature selection...\n",
      "Standardizing features...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load a sample of the data to reduce memory usage\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/processed/feature_matrix.csv').sample(frac=0.1, random_state=42)  # Load 10% of the data\n",
    "\n",
    "# Step 2: Reduce data precision to save memory\n",
    "print(\"Reducing data precision...\")\n",
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    df[col] = df[col].astype('float32')\n",
    "\n",
    "# Step 3: Separate features and target\n",
    "X = df.drop(columns=['discharge_diagnosis'])  # Replace 'discharge_diagnosis' with the actual target column name\n",
    "y = df['discharge_diagnosis']\n",
    "\n",
    "# Identify and drop non-numeric columns from X\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "print(f\"Dropping non-numeric columns: {non_numeric_columns}\")\n",
    "X = X.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Step 4: Feature selection to reduce dimensionality (remove low-variance features)\n",
    "print(\"Applying feature selection...\")\n",
    "selector = VarianceThreshold(threshold=0.01)  # Remove features with low variance\n",
    "X = selector.fit_transform(X)\n",
    "\n",
    "# Step 5: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Standardize features\n",
    "print(\"Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding target labels...\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Encode target labels if they are strings\n",
    "if y_train.dtype == 'object':\n",
    "    print(\"Encoding target labels...\")\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress warnings (optional)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")\n",
    "\n",
    "# Ensure no NaN or infinite values are present\n",
    "X_train = np.nan_to_num(X_train)\n",
    "y_train = np.nan_to_num(y_train)\n",
    "\n",
    "# Create an XGBoost classifier with optimized parameters\n",
    "model = xgb.XGBClassifier(\n",
    "    tree_method='hist',  # Use histogram-based method\n",
    "    gpu_id=0,  # Specify CUDA device\n",
    "    max_bin=64,  # Further reduce bin count\n",
    "    subsample=0.8,  # Subsample data for each tree\n",
    "    colsample_bytree=0.8,  # Use a fraction of features for each tree\n",
    "    n_estimators=30,  # Reduce number of boosting rounds\n",
    "    max_depth=4  # Lower tree depth to reduce memory usage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3213788/2523769275.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_3213788/2523769275.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_3213788/2523769275.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
      "/tmp/ipykernel_3213788/2523769275.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 7.92 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 7.82 GiB memory in use. Of the allocated memory 415.29 MiB is allocated by PyTorch, and 6.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[1;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 36\u001b[0m, in \u001b[0;36mImprovedNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n\u001b[1;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 7.92 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 7.82 GiB memory in use. Of the allocated memory 415.29 MiB is allocated by PyTorch, and 6.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Ensure y_train and y_test are integers\n",
    "if y_train.dtype == 'object' or y_test.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Improved neural network with additional layers, dropout, and batch normalization\n",
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, len(torch.unique(y_train)))  # Adjust output size based on number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ImprovedNN(X_train.shape[1]).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).argmax(dim=1)\n",
    "    accuracy = (predictions == y_test).float().mean().item()\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m trigger_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_epochs\u001b[49m):\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_test)\n",
    "        val_loss = criterion(val_outputs, y_test)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[02:03:37] /workspace/src/c_api/../common/device_helpers.cuh:393: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 500432896\n- Requested memory: 555481536\n\nStack trace:\n  [bt] (0) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x22dcbc) [0x78c2ea22dcbc]\n  [bt] (1) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x8a9966) [0x78c2ea8a9966]\n  [bt] (2) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xec59a) [0x78c2ea0ec59a]\n  [bt] (3) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x927a4d) [0x78c2ea927a4d]\n  [bt] (4) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x929058) [0x78c2ea929058]\n  [bt] (5) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x737fb5) [0x78c2ea737fb5]\n  [bt] (6) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x57bcd1) [0x78c2ea57bcd1]\n  [bt] (7) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x5cb03e) [0x78c2ea5cb03e]\n  [bt] (8) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x6f) [0x78c2ea13752f]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/xgboost/core.py:2100\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2100\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/xgboost/core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [02:03:37] /workspace/src/c_api/../common/device_helpers.cuh:393: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 500432896\n- Requested memory: 555481536\n\nStack trace:\n  [bt] (0) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x22dcbc) [0x78c2ea22dcbc]\n  [bt] (1) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x8a9966) [0x78c2ea8a9966]\n  [bt] (2) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xec59a) [0x78c2ea0ec59a]\n  [bt] (3) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x927a4d) [0x78c2ea927a4d]\n  [bt] (4) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x929058) [0x78c2ea929058]\n  [bt] (5) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x737fb5) [0x78c2ea737fb5]\n  [bt] (6) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x57bcd1) [0x78c2ea57bcd1]\n  [bt] (7) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x5cb03e) [0x78c2ea5cb03e]\n  [bt] (8) /home/zinger/miniconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x6f) [0x78c2ea13752f]\n\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Make predictions and evaluate\n",
    "print(\"Evaluating the model...\")\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Training\n",
    "# ----------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# def train_random_forest(X, y):\n",
    "#     \"\"\"\n",
    "#     Train a RandomForest model.\n",
    "#     \"\"\"\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#     model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     report = classification_report(y_test, y_pred)\n",
    "#     return model, accuracy, report\n",
    "\n",
    "# Load feature data\n",
    "feature_data = pd.read_csv('../data/processed/feature_matrix.csv')\n",
    "X = feature_data.drop(columns=['discharge_diagnosis', 'hadm_id'])  # Replace 'outcome' with actual target variable\n",
    "y = feature_data['discharge_diagnosis']  # Replace 'outcome' with actual target variable\n",
    "\n",
    "# Identify and drop non-numeric columns from X\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "print(f\"Dropping non-numeric columns: {non_numeric_columns}\")\n",
    "X = X.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load and split your data (assuming X and y are already defined)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode string labels to integers\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "\n",
    "# Delete variables you no longer need\n",
    "del X_train, X_test, y_train, y_test  # Adjust according to your variable names\n",
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an XGBoost classifier with GPU support and optimized parameters\n",
    "model = xgb.XGBClassifier(\n",
    "    tree_method='gpu_hist',  # Use GPU-accelerated training\n",
    "    use_label_encoder=False,\n",
    "    max_bin=256,  # Reduce bins to save memory\n",
    "    subsample=0.8,  # Subsample 80% of the data\n",
    "    colsample_bytree=0.8,  # Use 80% of features for each tree\n",
    "    n_estimators=50,  # Reduce the number of trees\n",
    "    max_depth=6  # Limit tree depth\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Transform predictions back to original string labels (optional, for interpretation)\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import gc\n",
    "\n",
    "# Load and split your data (ensure it's reduced or optimized)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reduce memory usage\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Remove constant columns if applicable\n",
    "constant_columns = [col for col in X_train.columns if X_train[col].nunique() <= 1]\n",
    "if constant_columns:\n",
    "    print(f\"Removing constant columns: {constant_columns}\")\n",
    "    X_train = X_train.drop(columns=constant_columns)\n",
    "    X_test = X_test.drop(columns=constant_columns)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "\n",
    "# Train LightGBM model with parameters to reduce memory consumption\n",
    "model = lgb.LGBMClassifier(device='gpu', max_bin=256, n_estimators=100, max_depth=6)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Ensure y_train and y_test are integers\n",
    "if y_train.dtype == 'object' or y_test.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Improved neural network with additional layers, dropout, and batch normalization\n",
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, len(torch.unique(y_train)))  # Adjust output size based on number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ImprovedNN(X_train.shape[1]).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).argmax(dim=1)\n",
    "    accuracy = (predictions == y_test).float().mean().item()\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, '../models/random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model again\n",
    "model, accuracy, report = train_random_forest(X, y)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, '../models/random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: SHAP Analysis\n",
    "# ---------------------\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_shap_analysis(model, X):\n",
    "    \"\"\"\n",
    "    Compute SHAP values for feature importance analysis.\n",
    "    \"\"\"\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    return explainer, shap_values\n",
    "\n",
    "# Load model and data\n",
    "model = joblib.load('../models/random_forest_model.pkl')\n",
    "X = pd.read_csv('../data/processed/feature_matrix.csv').drop(columns=['discharge_diagnosis', 'hadm_id'])  # Adjust as needed\n",
    "\n",
    "explainer, shap_values = run_shap_analysis(model, X)\n",
    "\n",
    "# Visualize SHAP values\n",
    "plt.title(\"SHAP Summary Plot\")\n",
    "shap.summary_plot(shap_values[1], X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
